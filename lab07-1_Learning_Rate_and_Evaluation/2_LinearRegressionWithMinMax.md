# Lab07-1. Learning Rate and Evaluation - (2)

## Non-normalized inputs

    import tensorflow as tf
    import numpy as np
    
    tf.set_random_seed(777)  # for reproducibility
    
    # Non-normalized training set
    xy = np.array(
        [
            [828.659973, 833.450012, 908100, 828.349976, 831.659973],
            [823.02002, 828.070007, 1828100, 821.655029, 828.070007],
            [819.929993, 824.400024, 1438100, 818.97998, 824.159973],
            [816, 820.958984, 1008100, 815.48999, 819.23999],
            [819.359985, 823, 1188100, 818.469971, 818.97998],
            [819, 823, 1198100, 816, 820.450012],
            [811.700012, 815.25, 1098100, 809.780029, 813.669983],
            [809.51001, 816.659973, 1398100, 804.539978, 809.559998]
        ]
    )
    
    x_data = xy[:, 0:-1]
    y_data = xy[:, [-1]]
    
    # placeholders for a tensor that will be always fed
    X = tf.placeholder(tf.float32, shape=[None, 4])
    Y = tf.placeholder(tf.float32, shape=[None, 1])
    
    W = tf.Variable(tf.random_normal([4, 1]), name='weight')
    b = tf.Variable(tf.random_normal([1]), name='bias')
    
    # Hypothesis
    hypothesis = tf.matmul(X, W) + b
    
    # Simplified cost/loss function
    cost = tf.reduce_mean(tf.square(hypothesis - Y))
    
    # Minimize
    optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)
    train = optimizer.minimize(cost)
    
    # Launch the graph in a session
    sess = tf.Session()
    # Initializes global variables in the graph
    sess.run(tf.global_variables_initializer())
    
    for step in range(101):
        cost_val, hy_val, _= sess.run([cost, hypothesis, train], feed_dict={X: x_data, Y: y_data})
        print(step, "Cost:", cost_val, "\nPrediction:", hy_val)
    
[return]

0 Cost: 2455327200000.0 

Prediction: [[-1104436.4]

 [-2224342.8]

 [-1749606.8]

 [-1226179.4]

 [-1445287.1]

 [-1457459.5]

 [-1335740.5]

 [-1700924.6]]

1 Cost: 2.69762e+27 

Prediction: [[3.6637149e+13]

 [7.3754336e+13]

 [5.8019879e+13]

 [4.0671629e+13]

 [4.7933685e+13]

 [4.8337135e+13]

 [4.4302659e+13]

 [5.6406091e+13]]

2 Cost: inf 

Prediction: [[-1.2143879e+21]

 [-2.4446870e+21]

 [-1.9231472e+21]

 [-1.3481161e+21]

 [-1.5888267e+21]

 [-1.6021996e+21]

 [-1.4684714e+21]

 [-1.8696560e+21]]

3 Cost: inf 

Prediction: [[4.0252522e+28]

 [8.1032447e+28]

 [6.3745308e+28]

 [4.4685124e+28]

 [5.2663807e+28]

 [5.3107068e+28]

 [4.8674461e+28]

 [6.1972262e+28]]

4 Cost: inf 

Prediction: [[-1.3342243e+36]

 [-2.6859301e+36]

 [-2.1129243e+36]

 [-1.4811488e+36]

 [-1.7456130e+36]

 [-1.7603054e+36]

 [-1.6133809e+36]

 [-2.0541546e+36]]

5 Cost: inf 

Prediction: [[inf]

 [inf]

 [inf]

 [inf]

 [inf]

 [inf]

 [inf]

 [inf]]

. . .

100 Cost: nan 


Prediction: [[nan]

 [nan]

 [nan]

 [nan]

 [nan]

 [nan]

 [nan]

 [nan]]
 
 ## Normalize inputs (Without any Package)
 
    def min_max_scaler(data):
        numerator = data - np.min(data, 0)
        denominator = np.max(data, 0) - np.min(data, 0)
        # noise term prevents the zero division
        return numerator / (denominator + 1e-7)
    
    
    # Non-normalized training set
    xy = np.array(
        [
            [828.659973, 833.450012, 908100, 828.349976, 831.659973],
            [823.02002, 828.070007, 1828100, 821.655029, 828.070007],
            [819.929993, 824.400024, 1438100, 818.97998, 824.159973],
            [816, 820.958984, 1008100, 815.48999, 819.23999],
            [819.359985, 823, 1188100, 818.469971, 818.97998],
            [819, 823, 1198100, 816, 820.450012],
            [811.700012, 815.25, 1098100, 809.780029, 813.669983],
            [809.51001, 816.659973, 1398100, 804.539978, 809.559998]
        ]
    )
    
    # very important. It does not work without it.
    xy = min_max_scaler(xy)
    print(xy)
    
[return]

[[0.99999999 0.99999999 0.         1.         1.        ]

 [0.70548491 0.70439552 1.         0.71881782 0.83755791]
 
 [0.54412549 0.50274824 0.57608696 0.606468   0.6606331 ]
 
 [0.33890353 0.31368023 0.10869565 0.45989134 0.43800918
 
 [0.51436    0.42582389 0.30434783 0.58504805 0.42624401]
 
 [0.49556179 0.42582389 0.31521739 0.48131134 0.49276137]
 
 [0.11436064 0.         0.20652174 0.22007776 0.18597238]
 
 [0.         0.07747099 0.5326087  0.         0.        ]]
 
--------------------------------------

0 Cost: 0.15230925 

Prediction: [[ 1.6346191 ]

 [ 0.06613699]
 
 [ 0.35008186]
  
 [ 0.6707252 ]
 
 [ 0.6113075 ]
 
 [ 0.61464405]
 
 [ 0.23171967]
 
 [-0.1372836 ]]

1Cost: 0.15230872 

Prediction: [[ 1.634618  ]

 [ 0.06613842]

 [ 0.35008252]

 [ 0.670725  ]

 [ 0.6113076 ]

 [ 0.6146442 ]

 [ 0.23171999]

 [-0.13728246]]

2 Cost: 0.15230817 

Prediction: [[ 1.6346169 ]

 [ 0.06613985]

 [ 0.35008317]

 [ 0.67072475]

 [ 0.61130774]

 [ 0.6146444 ]

 [ 0.23172033]

 [-0.13728121]]

. . .

98 Cost: 0.15225515 

Prediction: [[ 1.6345053 ]

 [ 0.06627801]

 [ 0.35014623]

 [ 0.67070615]

 [ 0.61131597]

 [ 0.61466026]

 [ 0.2317512 ]

 [-0.1371676 ]]

99 Cost: 0.15225461 

Prediction: [[ 1.6345041 ]

 [ 0.06627944]

 [ 0.35014686]

 [ 0.670706  ]

 [ 0.6113161 ]

 [ 0.61466044]

 [ 0.23175152]

 [-0.13716647]]

100 Cost: 0.15225405 

Prediction: [[ 1.6345029 ]

 [ 0.06628087]

 [ 0.35014752]

 [ 0.67070574]

 [ 0.6113161 ]

 [ 0.6146606 ]

 [ 0.23175186]

 [-0.13716528]]
 
 ## Normalize inputs (Using 'sklearn' Package)
 
    import sklearn.preprocessing as skp
    
    # Non-normalized training set
    xy = np.array(
        [
            [828.659973, 833.450012, 908100, 828.349976, 831.659973],
            [823.02002, 828.070007, 1828100, 821.655029, 828.070007],
            [819.929993, 824.400024, 1438100, 818.97998, 824.159973],
            [816, 820.958984, 1008100, 815.48999, 819.23999],
            [819.359985, 823, 1188100, 818.469971, 818.97998],
            [819, 823, 1198100, 816, 820.450012],
            [811.700012, 815.25, 1098100, 809.780029, 813.669983],
            [809.51001, 816.659973, 1398100, 804.539978, 809.559998]
        ]
    )
    
    # fit() vs. fit_transform()
    # fit(): 내부적으로 normalization에 필요한 값들을 저장
    # fit_transform(): fit과 transform(normalization 적용)을 한 번에 수행
    xy = skp.MinMaxScaler().fit_transform(xy)
    print(xy)

[return]

[[1.         1.         0.         1.         1.        ]

 [0.70548491 0.70439552 1.         0.71881783 0.83755792]
 
 [0.54412549 0.50274824 0.57608696 0.60646801 0.6606331 ]
 
 [0.33890353 0.31368023 0.10869565 0.45989134 0.43800918]
 
 [0.51436    0.4258239  0.30434783 0.58504805 0.42624401]
 

 [0.49556179 0.4258239  0.31521739 0.48131134 0.49276137]

 [0.11436064 0.         0.20652174 0.22007776 0.18597238]

 [0.         0.07747099 0.5326087  0.         0.        ]]

-------------------------------------------------------


0 Cost: 0.15230925 

Prediction: [[ 1.6346191 ]

 [ 0.06613699]

 [ 0.35008186]

 [ 0.6707252 ]

 [ 0.6113075 ]

 [ 0.61464405]

 [ 0.23171967]

 [-0.1372836 ]]

1 Cost: 0.15230872 

Prediction: [[ 1.634618  ]

 [ 0.06613842]

 [ 0.35008252]

 [ 0.670725  ]

 [ 0.6113076 ]

 [ 0.6146442 ]

 [ 0.23171999]

 [-0.13728246]]

2 Cost: 0.15230817 

Prediction: [[ 1.6346169 ]

 [ 0.06613985]

 [ 0.35008317]

 [ 0.67072475]

 [ 0.61130774]

 [ 0.6146444 ]

 [ 0.23172033]

 [-0.13728121]]

. . .


98 Cost: 0.15225515 

Prediction: [[ 1.6345053 ]

 [ 0.06627801]

 [ 0.35014623]

 [ 0.67070615]

 [ 0.61131597]

 [ 0.61466026]

 [ 0.2317512 ]

 [-0.1371676 ]]

99 Cost: 0.15225461 

Prediction: [[ 1.6345041 ]

 [ 0.06627944]

 [ 0.35014686]

 [ 0.670706  ]

 [ 0.6113161 ]

 [ 0.61466044]

 [ 0.23175152]

 [-0.13716647]]

100 Cost: 0.15225405 

Prediction: [[ 1.6345029 ]

 [ 0.06628087]

 [ 0.35014752]

 [ 0.67070574]

 [ 0.6113161 ]

 [ 0.6146606 ]

 [ 0.23175186]

 [-0.13716528]]
